{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLOT\n",
    "#Beautiful Soup is used for parsing the html code and finding elements within it to properly extract key pieces of data\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#Im using splinter to navigate the browser with an automated testing system, logging in, click buttons, and such...\n",
    "from splinter import Browser\n",
    "# Importing Pandas to read the CSV files\n",
    "import pandas as pd\n",
    "#==== May not need whatever is below =====\n",
    "# Importing date-time\n",
    "import datetime as datetime\n",
    "\n",
    "# importing time so Python can sleep, making net32 think we are not launching a denial of service attack\n",
    "import time \n",
    "\n",
    "# The python event scheduler\n",
    "import schedule \n",
    "\n",
    "\n",
    "### Eastern Time Class ###\n",
    "\n",
    "class EST5EDT(datetime.tzinfo):\n",
    "\n",
    "    def utcoffset(self, dt):\n",
    "        return datetime.timedelta(hours=-5) + self.dst(dt)\n",
    "\n",
    "    def dst(self, dt):\n",
    "        d = datetime.datetime(dt.year, 3, 8)        #2nd Sunday in March\n",
    "        self.dston = d + datetime.timedelta(days=6-d.weekday())\n",
    "        d = datetime.datetime(dt.year, 11, 1)       #1st Sunday in Nov\n",
    "        self.dstoff = d + datetime.timedelta(days=6-d.weekday())\n",
    "        if self.dston <= dt.replace(tzinfo=None) < self.dstoff:\n",
    "            return datetime.timedelta(hours=1)\n",
    "        else:\n",
    "            return datetime.timedelta(0)\n",
    "        \n",
    "###################################\n",
    "\n",
    "executable_path = {'executable_path':'chromedriver.exe'}\n",
    "browser = Browser('chrome',**executable_path, headless = False)\n",
    "\n",
    "monitor = 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b4c1659a8688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m## This bit of code brings me to the exact href for each individual clothing article, so i can collect the href's for each item\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mgridOfClothes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'medium-up--three-quarters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'grid__item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'grid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m## This is the amount of clothing articles on the current html page we are on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "while monitor == 'Y':\n",
    "    \n",
    "    currentTime = datetime.datetime.now(tz = EST5EDT()).strftime('%H:%M:%S')\n",
    "    \n",
    "    if currentTime >= '23:00:01':\n",
    "        monitor = \"N\"\n",
    "    else:\n",
    "        print(currentTime)\n",
    "\n",
    "url = \"https://juicestore.com/\"\n",
    "\n",
    "browser.visit(url)\n",
    "\n",
    "# Rendering the HTML Shop page \n",
    "html = browser.html\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "time.sleep(2)\n",
    "### CLicking the Account button to log into my account \n",
    "browser.find_by_text('Account').click()\n",
    "time.sleep(2)\n",
    "\n",
    "### Filling in log-in details\n",
    "## Creating XML paths for the browser to search through the elements i want to fill with login details\n",
    "logInPath = \"//input[@id = 'CustomerEmail']\"\n",
    "passwordPath = \"//input[@id = 'CustomerPassword']\"\n",
    "## Creating XML path for the browser to click the login button\n",
    "submitPath = \"//input[@value = 'Login']\"\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "######################################\n",
    "## Having the browser fill in my email\n",
    "\n",
    "browser.find_by_xpath(logInPath).fill('carlosr03@gmail.com')\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "## Having the browser fill in my password\n",
    "browser.find_by_xpath(passwordPath).fill('Anthropology1')\n",
    "\n",
    "time.sleep(1.5)\n",
    "######################################\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "\n",
    "## Hving the browser click the submit button\n",
    "browser.find_by_xpath(submitPath).click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "### Once on the account page, navigate to the shop page \n",
    "## Clicking the shop page by text\n",
    "browser.find_by_text(\"Shop\").click()\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "### Direct the browser to the Clot section of the store \n",
    "browser.visit('https://www.juicestore.com/collections/new-arrival')\n",
    "\n",
    "\n",
    "# Rendering the HTML new arrival page \n",
    "html = browser.html\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "\n",
    "## This bit of code brings me to the exact href for each individual clothing article, so i can collect the href's for each item\n",
    "gridOfClothes = page_soup.find_all('div', class_ = 'medium-up--three-quarters')[0].find_all('div', class_ = 'grid__item')[1].find_all('div', class_ = 'grid')[0]\n",
    "\n",
    "## This is the amount of clothing articles on the current html page we are on\n",
    "### Disclaimer: Since the site automatically filters it's clothing articles to 'latest', i feel as though it's not necessary\n",
    "## to collect all values of all clothes in other pages, and just to focus on the first page based on how the filter for the \n",
    "## store behaves\n",
    "amountOfArticlesOnPage = len(gridOfClothes.find_all('div', class_ = 'grid__item'))\n",
    "\n",
    "\n",
    "### For all the clothing articles on the page, extract all their href's and their \n",
    "for i in range(0,amountOfArticlesOnPage):\n",
    "    clothingArticleHref = gridOfClothes.find_all('div', class_ = 'grid__item')[i].find_all('div', class_ = 'product-card')[0].find_all('a', class_ = 'grid-view-item__link')[0]['href']\n",
    "\n",
    "\n",
    "    ### Removing all 27 chars from the string, as the pattern for all cothing article holds these first 27 characters, and the following change\n",
    "    ## this allows me to focus on the difference between items when looking for a particular item\n",
    "    removingFirst27 = clothingArticleHref[27:]\n",
    "\n",
    "\n",
    "    ##Ater removing thre first 27 characters in the URL string, i'm going to replace all \"-\" from the url with a space, so i can \n",
    "    ## pinpoint key words in a string\n",
    "    string = removingFirst27.replace(\"-\",\" \")\n",
    "\n",
    "    string = string.lower()\n",
    "\n",
    "    if \"rose\" in string and \"gold\" in string and \"air\" in string or \"force\" in string:\n",
    "        print(\"its in this href\")\n",
    "        print(clothingArticleHref)        \n",
    "        browser.click_link_by_href(clothingArticleHref)\n",
    "        break\n",
    "\n",
    "### Selecting the shoes\n",
    "\n",
    "## Rendering the HTML Shop page \n",
    "html = browser.html\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "containerWSizeQtyCart= page_soup.find_all(\"form\", id = \"product_form_4373019000890\")[0]\n",
    "\n",
    "sizePick = 'w us 9'\n",
    "\n",
    "## Finding the amount of shoe sizes available\n",
    "amtSizeAvail = len(containerWSizeQtyCart.find_all(\"div\", class_ = \"product-option\")[0].find_all(\"div\", class_ = \"product-option__option\"))\n",
    "\n",
    "## For every 'size' available, have them grouped and ready to be indexed\n",
    "size = containerWSizeQtyCart.find_all(\"div\", class_ = \"product-option\")[0].find_all(\"div\", class_ = \"product-option__option\")\n",
    "\n",
    "### For every size available, iterate through the range of sizes available\n",
    "for i in range(0,amtSizeAvail):\n",
    "    ## If the size available is the same as the size i picked (I want)\n",
    "    if size[i].text.strip().lower() == sizePick:        \n",
    "        ## click on the size and then break the loop so i can navigate to the check out page\n",
    "        browser.find_by_text(size[i].text.strip()).click()\n",
    "        break\n",
    "\n",
    "time.sleep(1.5)\n",
    "### Add to cart\n",
    "addToCartPath = \"//button[@id = 'AddToCart-product-template']\"\n",
    "browser.find_by_xpath(addToCartPath).click()\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "### CLicking the Check Out Button\n",
    "browser.find_by_css('.btn__checkout')[0].click()\n",
    "\n",
    "## On the shipping address page\n",
    "# Going to select my address from the dropdown menu right under \"Shipping Address\"\n",
    "\n",
    "\n",
    "# finding the drop down menu by xpath\n",
    "dropDownPath = \"//select[@data-trekkie-id = 'shipping_address_field']\"\n",
    "browser.find_by_xpath(dropDownPath)[0].click()\n",
    "\n",
    "## clicking my address on the drop down menu by xpath\n",
    "address = \"//option[@value = '3047406338106']\"\n",
    "browser.find_by_xpath(address).click()\n",
    "browser.find_by_xpath(dropDownPath)[0].click()\n",
    "\n",
    "### CLicking the Continue to Shipping button\n",
    "## Clicking the browser by finding the CSS (Class)\n",
    "browser.find_by_css('.step__footer__continue-btn').click()\n",
    "\n",
    "### Clicking the 'Continue To Payment' button\n",
    "paymentPath = \"//button[@data-trekkie-id = 'continue_to_payment_method_button']\"\n",
    "browser.find_by_xpath(paymentPath).click()\n",
    "\n",
    "## Rendering the HTML Shop page \n",
    "html = browser.html\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "### On the payment page\n",
    "## Filling out the card numebr field\n",
    "cardNumberPath = \"//iframe[@class = 'card-fields-iframe']\"\n",
    "browser.find_by_xpath(cardNumberPath).click()\n",
    "\n",
    "### getting the iframes ID name for the card number\n",
    "# len(page_soup.find_all(\"iframe\", class_ = \"card-fields-iframe\"))\n",
    "iframeidCardNumber = page_soup.find_all(\"iframe\", class_ = \"card-fields-iframe\")[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to fix below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iframeidCardNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with browser.get_iframe(0) as iframe:\n",
    "    iframe = iframe.find_by_id(iframeidCardNumber)\n",
    "    for key in iframe.type('222222', slowly = True):\n",
    "        iframe.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page_soup.find_all('iframe'))\n",
    "page_soup.find_all('iframe')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentTime = datetime.datetime.now(tz = EST5EDT()).strftime('%H:%M:%S')\n",
    "\n",
    "if currentTime >= '00:00:00':\n",
    "    browser.reload()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Having the bot create a list of all the items it has on for sale and taking the href.\n",
    "# ## So if the clots release, the clot are the only thing to release, it will be unfamiliar to the href\n",
    "# ## and the bot will navigate towards that href\n",
    "\n",
    "# # Rendering the HTML Shop page \n",
    "# html = browser.html\n",
    "# page_soup = soup(html, 'html.parser')\n",
    "\n",
    "# time.sleep(1.5)\n",
    "\n",
    "# ## This bit of code brings me to the exact href for each individual clothing article, so i can collect the href's for each item\n",
    "# gridOfClothes = page_soup.find_all('div', class_ = 'medium-up--three-quarters')[0].find_all('div', class_ = 'grid__item')[1].find_all('div', class_ = 'grid')[0]\n",
    "\n",
    "# ## This is the amount of clothing articles on the current html page we are on\n",
    "# ### Disclaimer: Since the site automatically filters it's clothing articles to 'latest', i feel as though it's not necessary\n",
    "# ## to collect all values of all clothes in other pages, and just to focus on the first page based on how the filter for the \n",
    "# ## store behaves\n",
    "# amountOfArticlesOnPage = len(gridOfClothes.find_all('div', class_ = 'grid__item'))\n",
    "\n",
    "# listOfHrefs = []\n",
    "\n",
    "# ### For all the clothing articles on the page, extract all their href's and their \n",
    "# for i in range(0,amountOfArticlesOnPage):\n",
    "#     clothingArticleHref = gridOfClothes.find_all('div', class_ = 'grid__item')[i].find_all('div', class_ = 'product-card')[0].find_all('a', class_ = 'grid-view-item__link')[0]['href']\n",
    "#     listOfHrefs.append(clothingArticleHref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridOfClothes.find_all('div', class_ = 'grid__item')[0].find_all('div', class_ = 'product-card')[0].find_all('a', class_ = 'grid-view-item__link')[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
